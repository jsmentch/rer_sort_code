{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949da161-fa5c-4a40-831c-e4f99c9ee01b",
   "metadata": {},
   "source": [
    "# Notebook to sort RER files into task->semester->item->file format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a5cfb-b686-4e0f-9aa2-fd80f03a662c",
   "metadata": {},
   "source": [
    "the inputs to this notebook are all RER files as of march 2024 (rer_missing) and the csv file with corresponding metadata for all of these files from Nuria 'rer_audios_descr.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ebfd130-6dae-4bc6-bac5-46b06269cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89a0b3db-8067-4bf7-b5b9-a44b78ab39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1657847/3887735639.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['Task Name'] = data['Task Name'].str.replace('z', '').str.replace('Paired Assoc. Learning', 'Paired Associate Learning').str.replace('Associates', 'Associate')\n"
     ]
    }
   ],
   "source": [
    "#load csv from Nuria into pandas dataframe\n",
    "    data=pd.read_csv('rer_audios_descr.csv')\n",
    "    #remove files that have NaN in the subjects column (their metadata is missing, these are mostly the 'test' junk files)\n",
    "    data.dropna(subset=['Student Tracking Id'], inplace=True) \n",
    "# fix the inconsistently named task names\n",
    "# remove \"(\" and everything after it\n",
    "data['Task Name'] = data['Task Name'].str.split('(').str[0]\n",
    "# remove \"-\" and everything after it\n",
    "data['Task Name'] = data['Task Name'].str.split('-').str[0]\n",
    "# remove \":\" and everything after it\n",
    "data['Task Name'] = data['Task Name'].str.split(':').str[0]\n",
    "# remove leading and trailing spaces\n",
    "data['Task Name'] = data['Task Name'].str.strip()\n",
    "# remove z typo, fix PAL spellings\n",
    "data['Task Name'] = data['Task Name'].str.replace('z', '').str.replace('Paired Assoc. Learning', 'Paired Associate Learning').str.replace('Associates', 'Associate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0bf4cf63-e7d0-40eb-9577-748963b04105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>Audio Response</th>\n",
       "      <th>Assessment Name</th>\n",
       "      <th>Task Name</th>\n",
       "      <th>Form Name</th>\n",
       "      <th>Item Name</th>\n",
       "      <th>Student Tracking Id</th>\n",
       "      <th>Grade</th>\n",
       "      <th>semester</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>20181024</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0003_a_D-11_18_...</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0003_a_D-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>z</td>\n",
       "      <td>DEL_0_S_0003_a_D</td>\n",
       "      <td>11_18_0_0002</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>20181024</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0002_a_D-11_18_...</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0002_a_D-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>z</td>\n",
       "      <td>DEL_0_S_0002_a_D</td>\n",
       "      <td>11_18_0_0002</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>20181024</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>z</td>\n",
       "      <td>DEL_0_S_0001_b_U</td>\n",
       "      <td>11_18_0_0001</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>20181024</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>z</td>\n",
       "      <td>DEL_0_S_0001_b_U</td>\n",
       "      <td>11_18_0_0002</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>20181024</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...</td>\n",
       "      <td>G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>z</td>\n",
       "      <td>DEL_0_S_0001_b_U</td>\n",
       "      <td>11_18_0_0011</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513382</th>\n",
       "      <td>513382</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>SCORE_MISSING</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0012_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0012_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Synonyms</td>\n",
       "      <td>z</td>\n",
       "      <td>SYN_0_S_0012_a_U</td>\n",
       "      <td>11_18_0_0062</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513384</th>\n",
       "      <td>513384</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>SCORE_MISSING</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0014_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0014_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Synonyms</td>\n",
       "      <td>z</td>\n",
       "      <td>SYN_0_S_0014_a_U</td>\n",
       "      <td>11_18_0_0062</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513385</th>\n",
       "      <td>513385</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>SCORE_MISSING</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0018_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0018_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Synonyms</td>\n",
       "      <td>z</td>\n",
       "      <td>SYN_0_S_0018_a_U</td>\n",
       "      <td>11_18_0_0062</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513386</th>\n",
       "      <td>513386</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>SCORE_MISSING</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0022_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0022_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Synonyms</td>\n",
       "      <td>z</td>\n",
       "      <td>SYN_0_S_0022_a_U</td>\n",
       "      <td>11_18_0_0062</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513388</th>\n",
       "      <td>513388</td>\n",
       "      <td>rer_missing</td>\n",
       "      <td>SCORE_MISSING</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0025_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1-Synonyms(KF1)-SYN_0_S_0025_a_U-11_18_...</td>\n",
       "      <td>G0_F1_D1</td>\n",
       "      <td>Synonyms</td>\n",
       "      <td>z</td>\n",
       "      <td>SYN_0_S_0025_a_U</td>\n",
       "      <td>11_18_0_0062</td>\n",
       "      <td>KG</td>\n",
       "      <td>Y1_fall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>453859 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0            0              1  \\\n",
       "14              14  rer_missing       20181024   \n",
       "15              15  rer_missing       20181024   \n",
       "16              16  rer_missing       20181024   \n",
       "17              17  rer_missing       20181024   \n",
       "18              18  rer_missing       20181024   \n",
       "...            ...          ...            ...   \n",
       "513382      513382  rer_missing  SCORE_MISSING   \n",
       "513384      513384  rer_missing  SCORE_MISSING   \n",
       "513385      513385  rer_missing  SCORE_MISSING   \n",
       "513386      513386  rer_missing  SCORE_MISSING   \n",
       "513388      513388  rer_missing  SCORE_MISSING   \n",
       "\n",
       "                                                        2  \\\n",
       "14      G0_F1_D1-Deletion(KF1)-DEL_0_S_0003_a_D-11_18_...   \n",
       "15      G0_F1_D1-Deletion(KF1)-DEL_0_S_0002_a_D-11_18_...   \n",
       "16      G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...   \n",
       "17      G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...   \n",
       "18      G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...   \n",
       "...                                                   ...   \n",
       "513382  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0012_a_U-11_18_...   \n",
       "513384  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0014_a_U-11_18_...   \n",
       "513385  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0018_a_U-11_18_...   \n",
       "513386  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0022_a_U-11_18_...   \n",
       "513388  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0025_a_U-11_18_...   \n",
       "\n",
       "                                           Audio Response Assessment Name  \\\n",
       "14      G0_F1_D1-Deletion(KF1)-DEL_0_S_0003_a_D-11_18_...        G0_F1_D1   \n",
       "15      G0_F1_D1-Deletion(KF1)-DEL_0_S_0002_a_D-11_18_...        G0_F1_D1   \n",
       "16      G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...        G0_F1_D1   \n",
       "17      G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...        G0_F1_D1   \n",
       "18      G0_F1_D1-Deletion(KF1)-DEL_0_S_0001_b_U-11_18_...        G0_F1_D1   \n",
       "...                                                   ...             ...   \n",
       "513382  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0012_a_U-11_18_...        G0_F1_D1   \n",
       "513384  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0014_a_U-11_18_...        G0_F1_D1   \n",
       "513385  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0018_a_U-11_18_...        G0_F1_D1   \n",
       "513386  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0022_a_U-11_18_...        G0_F1_D1   \n",
       "513388  G0_F1_D1-Synonyms(KF1)-SYN_0_S_0025_a_U-11_18_...        G0_F1_D1   \n",
       "\n",
       "       Task Name Form Name         Item Name Student Tracking Id Grade  \\\n",
       "14      Deletion         z  DEL_0_S_0003_a_D        11_18_0_0002    KG   \n",
       "15      Deletion         z  DEL_0_S_0002_a_D        11_18_0_0002    KG   \n",
       "16      Deletion         z  DEL_0_S_0001_b_U        11_18_0_0001    KG   \n",
       "17      Deletion         z  DEL_0_S_0001_b_U        11_18_0_0002    KG   \n",
       "18      Deletion         z  DEL_0_S_0001_b_U        11_18_0_0011    KG   \n",
       "...          ...       ...               ...                 ...   ...   \n",
       "513382  Synonyms         z  SYN_0_S_0012_a_U        11_18_0_0062    KG   \n",
       "513384  Synonyms         z  SYN_0_S_0014_a_U        11_18_0_0062    KG   \n",
       "513385  Synonyms         z  SYN_0_S_0018_a_U        11_18_0_0062    KG   \n",
       "513386  Synonyms         z  SYN_0_S_0022_a_U        11_18_0_0062    KG   \n",
       "513388  Synonyms         z  SYN_0_S_0025_a_U        11_18_0_0062    KG   \n",
       "\n",
       "       semester  \n",
       "14      Y1_fall  \n",
       "15      Y1_fall  \n",
       "16      Y1_fall  \n",
       "17      Y1_fall  \n",
       "18      Y1_fall  \n",
       "...         ...  \n",
       "513382  Y1_fall  \n",
       "513384  Y1_fall  \n",
       "513385  Y1_fall  \n",
       "513386  Y1_fall  \n",
       "513388  Y1_fall  \n",
       "\n",
       "[453859 rows x 12 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at the dataframe to see if it looks good\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6370ea97-5607-4def-b9da-95a475fa9a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blending', 'CELF_RS', 'Child Verbal Assent Form', 'Deletion', 'Digit Span', 'EVT', 'Growth Mindset Question', 'KTEA LWR', 'KTEA NWD', 'Letter Naming', 'Letter Naming Context', 'Letter Naming Isolation', 'Letter Sounds', 'Letter Sounds Context', 'Letter Sounds Isolated', 'Nonword Reading', 'Nonword Repetition', 'Oral Reading Fluency', 'Paired Associate Learning', 'Parent Technology Survey', 'Pearson Copyright', 'Picture Naming', 'Rapid Letter Naming', 'Rapid Naming', 'Rapid Object Naming', 'Sentence Repetition', 'Set for Variability', 'Synonyms', 'Visual Attention', 'WPPSI PN', 'Word Reading']\n"
     ]
    }
   ],
   "source": [
    "#print all of the unique tasks and make sure they look right\n",
    "unique_tasks = data['Task Name'].unique()\n",
    "print(sorted(unique_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cf5ea33-058d-437c-a1a4-135d74c5e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 of the tasks do not need item level subfolders. make a dataframe that incluedes these and another that excludes these\n",
    "tasks_to_exclude = ['Letter Naming Context', 'Letter Naming Isolation', 'Letter Sounds Context', \n",
    "                    'Letter Sounds Isolated', 'Rapid Letter Naming', 'Rapid Object Naming']\n",
    "# Dataframe with rows where 'Task Name' is not in the list of tasks to exclude\n",
    "df_standard = data[~data['Task Name'].isin(tasks_to_exclude)]\n",
    "# Dataframe with rows where 'Task Name' is in the list of tasks to exclude\n",
    "df_noitem = data[data['Task Name'].isin(tasks_to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df8b9577-ec0d-4f9d-8b51-6622904a8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the files that do not need to be placed in item specific folders (~7000 files, could take like 20 min)\n",
    "\n",
    "destination='/nese/mit/group/sig/projects/readnet/rer_sorted'\n",
    "# Iterate over the DataFrame rows\n",
    "for index, row in df_noitem.iterrows():\n",
    "    # Define the destination directory path\n",
    "    dest_dir = os.path.join(destination, row['Task Name'], row['semester'])\n",
    "        \n",
    "    src_file = os.path.join('/nese/mit/group/sig/projects/readnet/rer_missing/', str(row['1']), str(row['2']))\n",
    "\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy the file\n",
    "    shutil.copy(src_file, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73921057-aca0-4043-b542-27684e7ac0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the files that DO need to be placed in item specific folders (~400,000 files, could take like... 8-24+ hours depending on the file system etc)\n",
    "#NOTE this is actually broken out into the python script sort_rer.py now. It will run faster there instead of here.\n",
    "\n",
    "# destination='/nese/mit/group/sig/projects/readnet/rer_sorted'\n",
    "# # Iterate over the DataFrame rows\n",
    "# for index, row in df_standard.iterrows():\n",
    "#     # Define the destination directory path\n",
    "#     dest_dir = os.path.join(destination, row['Task Name'], row['semester'], row['Item Name'])\n",
    "        \n",
    "#     src_file = os.path.join('/nese/mit/group/sig/projects/readnet/rer_missing/', str(row['1']), str(row['2']))\n",
    "\n",
    "#     # Create the destination directory if it doesn't exist\n",
    "#     os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "#     # Copy the file\n",
    "#     shutil.copy(src_file, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8614e5f1-8838-4642-b989-acbd34343ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39121/1040383510.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.read_csv('rer_audios_descr.csv')\n"
     ]
    }
   ],
   "source": [
    "#move all files with nans in the columns (no metadata) to no_metadata folder\n",
    "data=pd.read_csv('rer_audios_descr.csv')\n",
    "nan_rows = data[pd.isna(data['Student Tracking Id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9172dafa-aed2-40ea-aa4c-832383155ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination='/nese/mit/group/sig/projects/readnet/rer_sorted/no_metadata/'\n",
    "for index, row in nan_rows.iterrows():\n",
    "    # Define the destination directory path        \n",
    "    src_file = os.path.join('/nese/mit/group/sig/projects/readnet/rer_missing/', str(row['1']), str(row['2']))\n",
    "    \n",
    "    # Copy the file\n",
    "    shutil.copy(src_file, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283eb04e-1532-49a1-a60e-fae017c9288f",
   "metadata": {},
   "source": [
    "# part 2, some files were not in the csv, move those files from no_metadata to the proper folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d59320-945b-4f7a-90e7-5bc43dc8841d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1015601/2102771427.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_new=pd.read_csv('rer_audios_descr_corrected_04_25.csv')\n",
      "/tmp/ipykernel_1015601/2102771427.py:18: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_new['Task Name'] = data_new['Task Name'].str.replace('z', '').str.replace('Paired Assoc. Learning', 'Paired Associate Learning').str.replace('Associates', 'Associate')\n"
     ]
    }
   ],
   "source": [
    "# load the updated csv\n",
    "data_new=pd.read_csv('rer_audios_descr_corrected_04_25.csv')\n",
    "#remove files that have NaN in the subjects column (their metadata is missing, these are mostly the 'test' junk files)\n",
    "data_new.dropna(subset=['Student Tracking Id'], inplace=True) \n",
    "# fix the inconsistently named task names\n",
    "data_new['Task Name'] = data_new['Task Name'].str.replace('ZFall2018 - ', '').str.replace('zFall2018 - ', '')\n",
    "data_new['Task Name'] = data_new['Task Name'].str.replace('letter naming 6 test', 'Letter Naming').str.replace('letter naming test 5 items', 'Letter Naming').str.replace('Letter naming test 6 items', 'Letter Naming')\n",
    "data_new['Task Name'] = data_new['Task Name'].str.replace('SRT test - why are options stupid', 'Sentence Repetition')\n",
    "# remove \"(\" and everything after it\n",
    "data_new['Task Name'] = data_new['Task Name'].str.split('(').str[0]\n",
    "# remove \"-\" and everything after it\n",
    "data_new['Task Name'] = data_new['Task Name'].str.split('-').str[0]\n",
    "# remove \":\" and everything after it\n",
    "data_new['Task Name'] = data_new['Task Name'].str.split(':').str[0]\n",
    "# remove leading and trailing spaces\n",
    "data_new['Task Name'] = data_new['Task Name'].str.strip()\n",
    "# remove z typo, fix PAL spellings\n",
    "data_new['Task Name'] = data_new['Task Name'].str.replace('z', '').str.replace('Paired Assoc. Learning', 'Paired Associate Learning').str.replace('Associates', 'Associate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b05a15b-a897-4cfa-8f04-774c453a4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the semesters to match this current convention\n",
    "mode_mapping = {\n",
    "    'Fall2018': 'Y1_fall',\n",
    "    'Fall2019': 'Y2_fall',\n",
    "    'Fall2020': 'Y3',\n",
    "    'Fall2021': 'Y4_fall',\n",
    "    'Fall2022': 'Y5_fall',\n",
    "    'Winter-Spring2019': 'Y1_winter',\n",
    "    'Winter-Spring2020': 'Y2_winter',\n",
    "    'Winter-Spring2021': 'Y3',\n",
    "    'Winter-Spring2022': 'Y4_winter',\n",
    "    'Winter-Spring 2023': 'Y5_winter',\n",
    "    'Winter-Spring2023': 'Y5_winter'\n",
    "}\n",
    "# Replace the old semester names with the new ones\n",
    "data_new['semester'] = data_new['semester'].replace(mode_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb69273-7950-4173-bcba-2f07e3f16ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Audio Response\n",
      "0      G0_F1_D1-Deletion(KF1)-DEL_0_S_0002_b_P-test_s...\n",
      "1      G0_F1_D1-Deletion(KF1)-DEL_0_S_0002_b_P-test_s...\n",
      "2      G0_F1_D1-LetterNaming-Context(K)-RAN_Upper_W-t...\n",
      "3      G0_F1_D1-LetterNaming-Context(K)-RAN_Upper_W-t...\n",
      "4      G0_F1_D1-LetterNaming-Context(K)-RAN_Upper_W-t...\n",
      "...                                                  ...\n",
      "60336  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "60337  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "60338  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "60339  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "60340  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "\n",
      "[60341 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#list all of the wav files in the no_metadata folder and save to dataframe\n",
    "import os\n",
    "import glob\n",
    "\n",
    "path = '/nese/mit/group/sig/projects/readnet/rer_sorted/no_metadata'\n",
    "extension = 'wav'\n",
    "\n",
    "os.chdir(path)\n",
    "result = glob.glob(f'*.{extension}')\n",
    "no_metadata = pd.DataFrame(result, columns=['Audio Response'])\n",
    "print(no_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab01676-df0e-4833-af37-f8718f572bae",
   "metadata": {},
   "source": [
    "right now there are 60k files in no_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36185775-1aa3-4ac5-b284-2c153273b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the rows of data_new that have the same 'Audio Response' as no_metadata.\n",
    "#these are the files we will try to move\n",
    "matching_rows = data_new[data_new['Audio Response'].isin(no_metadata['Audio Response'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d96c160e-af3f-4283-8739-cef07fdafbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the no_metadata files in item specific folders (~60k files, could take like... an hour or 2 max ?)\n",
    "\n",
    "destination='/nese/mit/group/sig/projects/readnet/rer_sorted'\n",
    "# Iterate over the DataFrame rows\n",
    "for index, row in matching_rows.iterrows():\n",
    "    # Define the destination directory path\n",
    "    dest_dir = os.path.join(destination, row['Task Name'], row['semester'], row['Item Name'])\n",
    "        \n",
    "    src_file = os.path.join('/nese/mit/group/sig/projects/readnet/rer_sorted/no_metadata/', str(row['2']))\n",
    "\n",
    "    #print(src_file, dest_dir)\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy the file\n",
    "    shutil.copy(src_file, dest_dir)\n",
    "    #remove the old file\n",
    "    os.remove(src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba9ad17f-cf71-4aa2-8437-8e22a0ef02f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Audio Response\n",
      "0                                638047146367380190.wav\n",
      "1                                638047146270344700.wav\n",
      "2                                638046406710939100.wav\n",
      "3                                638046406661012410.wav\n",
      "4                                638046406628054460.wav\n",
      "...                                                 ...\n",
      "5393  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "5394  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "5395  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "5396  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "5397  Winter22VirtualFidelityTestout-Part1-OralReadi...\n",
      "\n",
      "[5398 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#list all of the wav files in the no_metadata folder afterwards\n",
    "import os\n",
    "import glob\n",
    "\n",
    "path = '/nese/mit/group/sig/projects/readnet/rer_sorted/no_metadata'\n",
    "extension = 'wav'\n",
    "\n",
    "os.chdir(path)\n",
    "result = glob.glob(f'*.{extension}')\n",
    "post_no_metadata = pd.DataFrame(result, columns=['Audio Response'])\n",
    "print(post_no_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615573eb-8708-4821-8ad6-d29016c2bb41",
   "metadata": {},
   "source": [
    " now there are 5k files in no_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2996712-f09b-441f-a74a-f2eccacb979f",
   "metadata": {},
   "source": [
    "# part 3, make a csv file which lists the files of each directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebe3aaa3-dd8d-4c5e-96b8-a82a0fb43a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Specify the target directory path\n",
    "target_directory = \"/nese/mit/group/sig/projects/readnet/rer_sorted/\"\n",
    "\n",
    "# Get a list of all subdirectories\n",
    "subdirectories = [f.path for f in os.scandir(target_directory) if f.is_dir()]\n",
    "subdirectories = [os.path.basename(os.path.normpath(path)) for path in subdirectories]\n",
    "\n",
    "with open('all_files_2024-04-29.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for folder_name in subdirectories:\n",
    "        for root, dirs, files in os.walk(f'{target_directory}/{folder_name}'):\n",
    "            for file in files:\n",
    "                writer.writerow([folder_name, file])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
